{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b44c21-1c04-4e30-ac92-39cb9453639b",
   "metadata": {},
   "source": [
    "# Pre-process wflow_sbm forcing for CAMELS-GB in parallel\n",
    "## CEH-GEAR: pr, CHESS-PE: pet, CHESS-met: tas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5eb70a-f6f2-4ec7-8f7d-b5c2d9a668a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b71e39-8d5e-4a18-9bfd-247cc60ff8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import iris\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from esmvalcore.preprocessor import regrid\n",
    "from pathos.threading import ThreadPool as Pool\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866131c-6415-4284-ae3d-6517cb631aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snellius paths\n",
    "ROOT = Path('/gpfs/work1/0/wtrcycle/users/jaerts/camels_uk/')\n",
    "AUXDATA = Path(f\"{ROOT}/aux_data\")\n",
    "FORCING = Path(f'{ROOT}/forcing/')\n",
    "MODELS = Path(f'{ROOT}/wflow/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f230010d-cd9e-4b0e-8945-2f7845b52ea4",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19a632-03a8-4bab-b01c-4cf3f7bdbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Period\n",
    "start_year = \"2000\"\n",
    "end_year = \"2017\"\n",
    "\n",
    "# Get available basin IDs wflow_sbm\n",
    "basin_dirs = glob(f'{MODELS}/*')\n",
    "basin_ids = [s.split('/')[-1] for s in basin_dirs]\n",
    "basin_ids.sort()\n",
    "\n",
    "# Amount of available cores\n",
    "cores_available = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b6e2b-5214-4bc6-8d0f-4b584f87035b",
   "metadata": {},
   "source": [
    "# Preprocessor Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56df74a-51ce-4a54-bc95-a28d7e3bf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare forcing function\n",
    "def prep_forcing(basin_id, start_year, end_year):\n",
    "    print(basin_id)\n",
    "    # Create lists\n",
    "    output = []\n",
    "    output_da = []\n",
    "    \n",
    "    # Set basin directory\n",
    "    BASINDIR = f'{MODELS}/{basin_id}/'\n",
    "       \n",
    "    # Open netCDF file as an example grid from the model directory\n",
    "    cube_example = iris.load(f'{BASINDIR}/staticmaps.nc')[1]\n",
    "\n",
    "    # Guess bounds   \n",
    "    cube_example.coord('y').guess_bounds()\n",
    "    cube_example.coord('x').guess_bounds()\n",
    "    \n",
    "    # Rename Coords\n",
    "    cube_example.coord('y').rename('latitude')\n",
    "    cube_example.coord('x').rename('longitude')\n",
    "    \n",
    "    cube_example.coord('latitude').units = 'degrees'\n",
    "    cube_example.coord('longitude').units = 'degrees'\n",
    "    \n",
    "    # Loop forcing variables\n",
    "    for variable in ['pr','tas','pet']:\n",
    "\n",
    "        # Load forcing file\n",
    "        cube_forcing = iris.load(glob(f'{FORCING}/*{variable}*')[0])[0]\n",
    "        \n",
    "        # Guess bounds\n",
    "        cube_forcing.coord('latitude').guess_bounds()\n",
    "        cube_forcing.coord('longitude').guess_bounds()\n",
    "\n",
    "        # Regrid forcing file to example grid using conservative method\n",
    "        cube_out = regrid(cube_forcing, cube_example, scheme='area_weighted')\n",
    "        \n",
    "        # Rename Coords\n",
    "        cube_out.coord('latitude').rename('lat')\n",
    "        cube_out.coord('longitude').rename('lon')\n",
    "        \n",
    "        # Convert to xarray and append to list\n",
    "        da = xr.DataArray.from_iris(cube_out)\n",
    "        output_da.append(da)\n",
    "    \n",
    "    # Change annoying long_name time\n",
    "    for da in output_da:\n",
    "        da.time.attrs = {'standard_name': 'time',\n",
    "                         'long_name': 'time in days since 1961-01-01 00:00:00 UTC'}\n",
    "        output.append(da)\n",
    "    \n",
    "    # Merge output variables\n",
    "    ds = xr.merge(output)#, dim='time')\n",
    "    ds = ds.rename({'lat': 'y', 'lon':'x', 'pr':'precip','tas': 'temp'})\n",
    "    \n",
    "    # Output filename\n",
    "    output_fname = f'{BASINDIR}/ceh-gear_chess_camels-gb_{basin_id}_2000_2017.nc'\n",
    "    \n",
    "    # Remove existing file\n",
    "    if output_fname:\n",
    "        OUTPUT = Path(output_fname)\n",
    "        OUTPUT.unlink(output_fname)\n",
    "        \n",
    "    # Save to netcdf\n",
    "    write_job = ds.to_netcdf(output_fname, compute=False)\n",
    "    with ProgressBar():\n",
    "        write_job.compute()\n",
    "\n",
    "    # ds.to_netcdf(output_fname)\n",
    "    return print(f'{basin_id} finished: {output_fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c0a7c-4d6f-4839-af45-70692895b87c",
   "metadata": {},
   "source": [
    "# Parallel Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06cb16-bd4b-4694-9cb0-26df7cd66da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_run(\n",
    "    basin_ids,\n",
    "    start_years,\n",
    "    end_years, \n",
    "    threads=cores_available,\n",
    "    ):\n",
    "    \n",
    "    # Set number of threads (cores) used for parallel run and map threads\n",
    "    if threads is None:\n",
    "        pool = Pool()\n",
    "    else:\n",
    "        pool = Pool(nodes=threads)\n",
    "    # Run parallel models\n",
    "    pool.map(\n",
    "        prep_forcing,\n",
    "        basin_ids,\n",
    "        start_years,\n",
    "        end_years,\n",
    "        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa2f24-469e-44b5-b6ef-8288fa0e7c23",
   "metadata": {},
   "source": [
    "## Sort basins by size for lazy parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9788b20-adb8-4ee4-90f3-13a6a6655c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by basin size\n",
    "def sort_basin_ids_by_size(basin_ids):\n",
    "    sizes = []\n",
    "    for basin_id in basin_ids:\n",
    "        size = os.path.getsize(f'{MODELS}/{basin_id}/staticmaps.nc')\n",
    "        sizes.append(size)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['basin_id'] = basin_ids\n",
    "    df['size'] = sizes\n",
    "    df = df.sort_values('size')\n",
    "\n",
    "    basin_ids = df.basin_id.to_list()\n",
    "    \n",
    "    return basin_ids\n",
    "\n",
    "basin_ids_sorted = sort_basin_ids_by_size(basin_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584085a-f46c-4f90-8912-d77cd6c7525c",
   "metadata": {},
   "source": [
    "## Create lists for lazy parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718cb6a-1aa7-49c6-a3af-4cd23c6a1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for parallel runs\n",
    "start_years = [start_year] * len(basin_ids_sorted)\n",
    "end_years = [end_year] * len(basin_ids_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fffad4-f616-4fa1-851d-ef296068525c",
   "metadata": {},
   "source": [
    "# Run parallel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea25cc-9da6-4c41-abe8-2ae4323f8ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run function\n",
    "parallel_run(basin_ids_sorted, start_years, end_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e70017-d926-4ce9-bc74-d73e0dda3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_run(basin_ids_sorted, start_years, end_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e817be-57d9-444f-8142-0fb0d3c75408",
   "metadata": {},
   "source": [
    "# Check completed runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2719f9fd-9ff5-41c9-ac98-91d593fcb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "basins = []\n",
    "exists = []\n",
    "\n",
    "for basin_id in basin_ids_sorted:\n",
    "    basins.append(basin_id)\n",
    "   \n",
    "    # check if file exists\n",
    "    file = Path(f'{MODELS}/{basin_id}/ceh-gear_chess_camels-gb_{basin_id}_2000_2017.nc')\n",
    "    exists.append(file.is_file())\n",
    "    \n",
    "df['basin_id'] = basins\n",
    "df['completed'] = exists\n",
    "df = df[df['completed'] == False]    \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d41e8-f897-45dc-b1a8-66085c4a5753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for basin_id in temp:\n",
    "    prep_forcing(basin_id, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc2561-4860-4a4e-9253-94cd9bd4304d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
