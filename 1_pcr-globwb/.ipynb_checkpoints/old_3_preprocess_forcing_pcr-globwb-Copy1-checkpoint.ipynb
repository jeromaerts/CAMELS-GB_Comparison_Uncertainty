{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b44c21-1c04-4e30-ac92-39cb9453639b",
   "metadata": {},
   "source": [
    "# Pre-process pcr-globwb forcing for CAMELS-GB in parallel\n",
    "## CEH-GEAR: pr, CHESS-PE: pet, CHESS-met: tas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5eb70a-f6f2-4ec7-8f7d-b5c2d9a668a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b71e39-8d5e-4a18-9bfd-247cc60ff8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import iris\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio\n",
    "import rioxarray\n",
    "\n",
    "from esmvalcore.preprocessor import regrid\n",
    "from pathos.threading import ThreadPool as Pool\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5866131c-6415-4284-ae3d-6517cb631aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snellius paths\n",
    "ROOT = Path('/gpfs/work1/0/wtrcycle/users/jaerts/camels_uk/')\n",
    "AUXDATA = Path(f\"{ROOT}/aux_data\")\n",
    "FORCING = Path(f'{ROOT}/forcing/')\n",
    "MODELS = Path(f'{ROOT}/pcr-globwb/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f230010d-cd9e-4b0e-8945-2f7845b52ea4",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac19a632-03a8-4bab-b01c-4cf3f7bdbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Period\n",
    "start_year = \"2000\"\n",
    "end_year = \"2017\"\n",
    "\n",
    "# Get available basin IDs wflow_sbm\n",
    "basin_dirs = glob(f'{MODELS}/*')\n",
    "basin_ids = [s.split('/')[-1] for s in basin_dirs]\n",
    "basin_ids.remove('uk')\n",
    "basin_ids.sort()\n",
    "\n",
    "# Amount of available cores\n",
    "cores_available = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b6e2b-5214-4bc6-8d0f-4b584f87035b",
   "metadata": {},
   "source": [
    "# Preprocessor Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c56df74a-51ce-4a54-bc95-a28d7e3bf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare forcing function\n",
    "def prep_forcing(basin_id, start_year, end_year):\n",
    "    print(basin_id)\n",
    "    # Set basin directory\n",
    "    BASINDIR = f'{MODELS}/{basin_id}/'\n",
    "\n",
    "    # Open netCDF file as an example grid from the model directory\n",
    "    ds = xr.open_rasterio(f'{BASINDIR}/{basin_id}_30sec_clone.map')\n",
    "    cube_example = ds.squeeze('band').drop('band').to_iris()\n",
    "\n",
    "    cube_example.coord('y').rename('latitude')\n",
    "    cube_example.coord('x').rename('longitude')\n",
    "\n",
    "    # Guess bounds   \n",
    "    cube_example.coord('latitude').guess_bounds()\n",
    "    cube_example.coord('longitude').guess_bounds()\n",
    "\n",
    "    cube_example.coord('latitude').units = 'degrees'\n",
    "    cube_example.coord('longitude').units = 'degrees'\n",
    "\n",
    "    # Loop forcing variables\n",
    "    for variable in ['pr','tas','pet']:\n",
    "        print(variable)\n",
    "\n",
    "        # Load forcing file\n",
    "        cube_forcing = xr.open_dataset(glob(f'{FORCING}/*{variable}*')[0])[variable].to_iris()\n",
    "\n",
    "        # Guess bounds\n",
    "        cube_forcing.coord('latitude').guess_bounds()\n",
    "        cube_forcing.coord('longitude').guess_bounds()\n",
    "\n",
    "        # Regrid forcing file to example grid using conservative method\n",
    "        cube_out = regrid(cube_forcing, cube_example, scheme='linear')\n",
    "\n",
    "        # Rename Coords\n",
    "        cube_out.coord('latitude').rename('lat')\n",
    "        cube_out.coord('longitude').rename('lon')\n",
    "\n",
    "        # Convert to xarray\n",
    "        da = xr.DataArray.from_iris(cube_out)\n",
    "\n",
    "        # Create climatology\n",
    "        da_clim = da.sel(time=slice('2000', '2007'))\n",
    "        da_clim = da_clim.convert_calendar('365_day')\n",
    "        da_clim = da_clim.groupby(\"time.dayofyear\").mean('time')\n",
    "        da_clim = da_clim.assign_coords(dayofyear=xr.date_range('2007-01-01','2007-12-31'))\n",
    "        da_clim = da_clim.rename({'dayofyear':'time'})\n",
    "\n",
    "        # Slice dataarray and concat\n",
    "        da = da.sel(time=slice('2008', '2017'))\n",
    "        da = xr.concat([da_clim, da], dim='time')\n",
    "\n",
    "        # Set attributes\n",
    "        da.lon.attrs = {'long_name': 'longitude',\n",
    "                        'standard_name': 'longitude',\n",
    "                        'units': 'degrees'}\n",
    "        da.lat.attrs = {'long_name': 'latitude',\n",
    "                        'standard_name': 'latitude',\n",
    "                        'units': 'degrees'}   \n",
    "        da.time.attrs = {'standard_name': 'time',\n",
    "                         'long_name': 'time'}\n",
    "        # Convert to dataset\n",
    "        da = da.to_dataset()\n",
    "        \n",
    "        # convert to m*day\n",
    "        if variable == 'pr':\n",
    "            da = da * 0.001\n",
    "        if variable == 'pet':\n",
    "            da = da * 0.001\n",
    "        if variable == 'tas':\n",
    "            pass\n",
    "        \n",
    "        # Output filename\n",
    "        output_fname = f'{BASINDIR}/ceh-gear_chess_camels-gb_{basin_id}_{variable}_clim2000-2007_2017.nc'\n",
    "\n",
    "        # Remove existing file\n",
    "        if output_fname:\n",
    "            OUTPUT = Path(output_fname)\n",
    "            OUTPUT.unlink(output_fname)\n",
    "\n",
    "        # Save to netcdf\n",
    "        write_job = da.to_netcdf(output_fname, encoding={f'{variable}': {'_FillValue': -9999, 'missing_value':-9999}}, compute=False)\n",
    "        with ProgressBar():\n",
    "            write_job.compute()\n",
    "\n",
    "\n",
    "    return print(f'{basin_id} finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea47ebd1-5d56-4d63-aee1-414ecd8298fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # loop basins\n",
    "# for basin_id in basin_ids:\n",
    "#     prep_forcing(basin_id, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaad0c3a-0dba-4134-a7b2-f1447750377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45005\n",
      "pr\n",
      "[########################################] | 100% Completed | 101.89 ms\n",
      "tas\n",
      "[########################################] | 100% Completed | 103.36 ms\n",
      "pet\n",
      "[########################################] | 100% Completed | 101.99 ms\n",
      "45005 finished\n"
     ]
    }
   ],
   "source": [
    "# Temp TEST!\n",
    "basin_id = '45005'\n",
    "prep_forcing(basin_id, start_year, end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b5eb9-bb7c-4be8-8d49-2b2f8a540c55",
   "metadata": {},
   "source": [
    "# UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "938bfe74-eb17-4e48-ab60-7eb6372ac592",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_id = 'uk'\n",
    "# prep_forcing(basin_id, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c7f097b-8e23-44e3-8734-b6d9b3df0ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pet\n",
      "Regridding...\n",
      "[########################################] | 100% Completed | 102.51 ms\n",
      "tas\n",
      "Regridding...\n",
      "[########################################] | 100% Completed | 102.25 ms\n",
      "pr\n",
      "Regridding...\n",
      "[########################################] | 100% Completed | 102.24 ms\n"
     ]
    }
   ],
   "source": [
    "# Set basin directory\n",
    "BASINDIR = f'{MODELS}/{basin_id}/'\n",
    "\n",
    "# Open netCDF file as an example grid from the model directory\n",
    "ds = xr.open_rasterio(f'{BASINDIR}/{basin_id}_30sec_clone.map')\n",
    "cube_example = ds.squeeze('band').drop('band').to_iris()\n",
    "\n",
    "cube_example.coord('y').rename('latitude')\n",
    "cube_example.coord('x').rename('longitude')\n",
    "\n",
    "# Guess bounds   \n",
    "cube_example.coord('latitude').guess_bounds()\n",
    "cube_example.coord('longitude').guess_bounds()\n",
    "\n",
    "cube_example.coord('latitude').units = 'degrees'\n",
    "cube_example.coord('longitude').units = 'degrees'\n",
    "\n",
    "# Loop forcing variables\n",
    "for variable in ['pet','tas','pr']:\n",
    "    print(variable)\n",
    "\n",
    "    # Load forcing file\n",
    "    da_clim = xr.open_dataset(glob(f'{FORCING}/*{variable}*')[0])[variable]\n",
    "\n",
    "    # Create climatology\n",
    "    da_clim = da_clim.sel(time=slice('2000', '2007'))\n",
    "    da_clim = da_clim.convert_calendar('365_day')\n",
    "    da_clim = da_clim.groupby(\"time.dayofyear\").mean('time')\n",
    "    da_clim = da_clim.assign_coords(dayofyear=xr.date_range('2007-01-01','2007-12-31'))\n",
    "    da_clim = da_clim.rename({'dayofyear':'time'})\n",
    "\n",
    "    # Convert to cube\n",
    "    cube_forcing = da_clim.to_iris()\n",
    "\n",
    "    # Guess bounds\n",
    "    cube_forcing.coord('latitude').guess_bounds()\n",
    "    cube_forcing.coord('longitude').guess_bounds()\n",
    "\n",
    "    # Regrid forcing file to example grid using conservative method\n",
    "    print('Regridding...')\n",
    "    cube_out = regrid(cube_forcing, cube_example, scheme='area_weighted')\n",
    "\n",
    "    # Rename Coords\n",
    "    cube_out.coord('latitude').rename('lat')\n",
    "    cube_out.coord('longitude').rename('lon')\n",
    "\n",
    "    # Convert to xarray\n",
    "    da_clim = xr.DataArray.from_iris(cube_out)\n",
    "\n",
    "    cube_forcing = None\n",
    "    cube_out = None\n",
    "    \n",
    "    \n",
    "    # Set attributes\n",
    "    da_clim.lon.attrs = {'long_name': 'longitude',\n",
    "                    'standard_name': 'longitude',\n",
    "                    'units': 'degrees'}\n",
    "    da_clim.lat.attrs = {'long_name': 'latitude',\n",
    "                    'standard_name': 'latitude',\n",
    "                    'units': 'degrees'}   \n",
    "    da_clim.time.attrs = {'standard_name': 'time',\n",
    "                     'long_name': 'time'}\n",
    "    # Convert to dataset\n",
    "    da_clim = da_clim.to_dataset()\n",
    "\n",
    "    # convert to m*day\n",
    "    if variable == 'pr':\n",
    "        da_clim = da_clim * 0.001\n",
    "    if variable == 'pr':\n",
    "        da_clim = da_clim * 0.001\n",
    "    if variable == 'tas':\n",
    "        pass\n",
    "\n",
    "    # Output filename\n",
    "    output_fname = f'{BASINDIR}/ceh-gear_chess_camels-gb_{basin_id}_{variable}_clim2000-2007.nc'\n",
    "\n",
    "    # Remove existing file\n",
    "    if output_fname:\n",
    "        OUTPUT = Path(output_fname)\n",
    "        OUTPUT.unlink(output_fname)\n",
    "\n",
    "    # Save to netcdf\n",
    "    write_job = da_clim.to_netcdf(output_fname, encoding={f'{variable}': {'_FillValue': -9999, 'missing_value':-9999}}, compute=False)\n",
    "    with ProgressBar():\n",
    "        write_job.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd058f6-4db6-4478-bc64-af8a5e7cfb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pet\n",
      "2007\n",
      "[########################################] | 100% Completed | 102.00 ms\n",
      "2008\n",
      "[########################################] | 100% Completed | 102.77 ms\n",
      "2009\n",
      "[########################################] | 100% Completed | 104.75 ms\n",
      "2010\n",
      "[########################################] | 100% Completed | 102.47 ms\n",
      "2011\n",
      "[########################################] | 100% Completed | 109.16 ms\n",
      "2012\n",
      "[########################################] | 100% Completed | 102.14 ms\n",
      "2013\n",
      "[########################################] | 100% Completed | 103.28 ms\n",
      "2014\n",
      "[########################################] | 100% Completed | 102.48 ms\n",
      "2015\n",
      "[########################################] | 100% Completed | 103.78 ms\n",
      "2016\n",
      "[########################################] | 100% Completed | 102.37 ms\n",
      "2017\n",
      "[########################################] | 100% Completed | 102.49 ms\n",
      "tas\n",
      "2007\n",
      "[########################################] | 100% Completed | 102.07 ms\n",
      "2008\n",
      "[########################################] | 100% Completed | 102.28 ms\n",
      "2009\n",
      "[########################################] | 100% Completed | 102.56 ms\n",
      "2010\n",
      "[########################################] | 100% Completed | 102.19 ms\n",
      "2011\n",
      "[########################################] | 100% Completed | 102.12 ms\n",
      "2012\n",
      "[########################################] | 100% Completed | 101.81 ms\n",
      "2013\n",
      "[########################################] | 100% Completed | 102.32 ms\n",
      "2014\n",
      "[########################################] | 100% Completed | 102.46 ms\n",
      "2015\n",
      "[########################################] | 100% Completed | 102.21 ms\n",
      "2016\n",
      "[########################################] | 100% Completed | 102.11 ms\n",
      "2017\n",
      "[########################################] | 100% Completed | 102.42 ms\n",
      "pr\n",
      "2007\n",
      "[########################################] | 100% Completed | 102.50 ms\n",
      "2008\n",
      "[########################################] | 100% Completed | 108.71 ms\n",
      "2009\n",
      "[########################################] | 100% Completed | 102.45 ms\n",
      "2010\n",
      "[########################################] | 100% Completed | 102.18 ms\n",
      "2011\n",
      "[########################################] | 100% Completed | 102.36 ms\n",
      "2012\n",
      "[########################################] | 100% Completed | 102.31 ms\n",
      "2013\n",
      "[########################################] | 100% Completed | 102.25 ms\n",
      "2014\n",
      "[########################################] | 100% Completed | 102.16 ms\n",
      "2015\n",
      "[########################################] | 100% Completed | 102.19 ms\n",
      "2016\n",
      "[########################################] | 100% Completed | 102.07 ms\n",
      "2017\n",
      "[########################################] | 100% Completed | 102.12 ms\n"
     ]
    }
   ],
   "source": [
    "# Set basin directory\n",
    "BASINDIR = f'{MODELS}/{basin_id}/'\n",
    "\n",
    "# Open netCDF file as an example grid from the model directory\n",
    "ds = xr.open_rasterio(f'{BASINDIR}/{basin_id}_30sec_clone.map')\n",
    "cube_example = ds.squeeze('band').drop('band').to_iris()\n",
    "\n",
    "cube_example.coord('y').rename('latitude')\n",
    "cube_example.coord('x').rename('longitude')\n",
    "\n",
    "# Guess bounds   \n",
    "cube_example.coord('latitude').guess_bounds()\n",
    "cube_example.coord('longitude').guess_bounds()\n",
    "\n",
    "cube_example.coord('latitude').units = 'degrees'\n",
    "cube_example.coord('longitude').units = 'degrees'\n",
    "\n",
    "# Loop forcing variables\n",
    "for variable in ['pet','tas', 'pr']:\n",
    "    print(variable)\n",
    "    years = list(range(2007,2018))\n",
    "    # years = [2008]\n",
    "    for year in years:\n",
    "        print(year)\n",
    "        da = xr.open_dataset(glob(f'{FORCING}/*{variable}*')[0])[variable]\n",
    "        da = da.sel(time=str(year))\n",
    "        cube_forcing = da.to_iris()\n",
    "       \n",
    "        # Guess bounds\n",
    "        cube_forcing.coord('latitude').guess_bounds()\n",
    "        cube_forcing.coord('longitude').guess_bounds()\n",
    "\n",
    "        # Regrid forcing file to example grid using conservative method\n",
    "        cube_out = regrid(cube_forcing, cube_example, scheme='area_weighted')\n",
    "\n",
    "        # Rename Coords\n",
    "        cube_out.coord('latitude').rename('lat')\n",
    "        cube_out.coord('longitude').rename('lon')\n",
    "\n",
    "        # Convert to xarray\n",
    "        da = xr.DataArray.from_iris(cube_out)\n",
    "        \n",
    "        # Set attributes\n",
    "        da.lon.attrs = {'long_name': 'longitude',\n",
    "                        'standard_name': 'longitude',\n",
    "                        'units': 'degrees'}\n",
    "        da.lat.attrs = {'long_name': 'latitude',\n",
    "                        'standard_name': 'latitude',\n",
    "                        'units': 'degrees'}   \n",
    "        da.time.attrs = {'standard_name': 'time',\n",
    "                         'long_name': 'time'}\n",
    "        # Convert to dataset\n",
    "        da = da.to_dataset()\n",
    "        \n",
    "        # convert to m*day\n",
    "        if variable == 'pr':\n",
    "            da = da * 0.001\n",
    "        if variable == 'pet':\n",
    "            da = da * 0.001\n",
    "        if variable == 'tas':\n",
    "            pass\n",
    "        \n",
    "        # Output filename\n",
    "        output_fname = f'{BASINDIR}/ceh-gear_chess_camels-gb_{basin_id}_{variable}_{year}.nc'\n",
    "\n",
    "        # Remove existing file\n",
    "        if output_fname:\n",
    "            OUTPUT = Path(output_fname)\n",
    "            OUTPUT.unlink(output_fname)\n",
    "\n",
    "        # Save to netcdf\n",
    "        write_job = da.to_netcdf(output_fname, encoding={f'{variable}': {'_FillValue': -9999, 'missing_value':-9999}}, compute=False)\n",
    "        with ProgressBar():\n",
    "            write_job.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b14ce7-3791-46b7-b8c1-e3b96cb15181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
